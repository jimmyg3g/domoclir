% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/01.R
\name{dsUpload}
\alias{dsUpload}
\title{Dataset Upload}
\usage{
dsUpload(
  data,
  ds_id,
  headers = TRUE,
  ds_name = NULL,
  schema_file = NULL,
  type = NULL,
  append = NULL
)
}
\arguments{
\item{data}{either a \code{csv} file or a \code{data.frame}}

\item{ds_id}{dataset id}

\item{headers}{data file has a header row}

\item{ds_name}{dataset name}

\item{schema_file}{filename for schema definition, if a schema-file is provided, then Domo creates a new dataset, hrm.}

\item{type}{dataset type}

\item{append}{append to existing data (this options is required when doing Upserts or Partitions)}
}
\value{

}
\description{
upload-dataset: Uploads data to a Domo DataSet. An advantage of the CLI over workbench is that the CLI supports a model where if you can query the source in a way that unloads the result of the query to multiple CSV files, you can then directly push those csv parts in parallel to Domo.  If the CSV part files are precisely formatted according to the Domo spec, we donâ€™t need to read each row as the data is being sent. Workbench has to read every cell so that it can format the data to be sent.  A pipeline setup using an unload method into many parts can take 1/10 of the time vs the row-at-a-time model that workbench uses.
}
